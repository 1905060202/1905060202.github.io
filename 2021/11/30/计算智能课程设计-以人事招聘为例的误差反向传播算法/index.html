<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>计算智能课程设计(以人事招聘为例的误差反向传播算法) | 胡小宁的博客</title><meta name="keywords" content="人工智能,课程设计,计算智能"><meta name="author" content="胡小宁"><meta name="copyright" content="胡小宁"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="写在前面昨天写了基于感知机的鸢尾花分类，今天下午要考《大数据技术原理》，本来是整个白天的课程设计，因为考试少了一下午。之前看书复习了一波，但记忆是需要反复锤炼的，所以要抓紧写完这个传播算法，抽出时间再去复习会。2021.11.30&#x2F;2021.12.1&#x2F;2021.12.2 以人事招聘为例的误差反向传播算法实验目的理解多层神经网络的结构和原理，掌握反向传播算法对神经元的训练过程，了解反向传播公式。通过">
<meta property="og:type" content="article">
<meta property="og:title" content="计算智能课程设计(以人事招聘为例的误差反向传播算法)">
<meta property="og:url" content="http://1905060202.github.io/2021/11/30/%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1-%E4%BB%A5%E4%BA%BA%E4%BA%8B%E6%8B%9B%E8%81%98%E4%B8%BA%E4%BE%8B%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="胡小宁的博客">
<meta property="og:description" content="写在前面昨天写了基于感知机的鸢尾花分类，今天下午要考《大数据技术原理》，本来是整个白天的课程设计，因为考试少了一下午。之前看书复习了一波，但记忆是需要反复锤炼的，所以要抓紧写完这个传播算法，抽出时间再去复习会。2021.11.30&#x2F;2021.12.1&#x2F;2021.12.2 以人事招聘为例的误差反向传播算法实验目的理解多层神经网络的结构和原理，掌握反向传播算法对神经元的训练过程，了解反向传播公式。通过">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://1905060202.github.io/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg">
<meta property="article:published_time" content="2021-11-30T00:19:23.000Z">
<meta property="article:modified_time" content="2021-12-02T05:06:52.878Z">
<meta property="article:author" content="胡小宁">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="课程设计">
<meta property="article:tag" content="计算智能">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://1905060202.github.io/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://1905060202.github.io/2021/11/30/%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1-%E4%BB%A5%E4%BA%BA%E4%BA%8B%E6%8B%9B%E8%81%98%E4%B8%BA%E4%BE%8B%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"6456TWTNI2","apiKey":"cf5116dbc73ddb4c9ee08cca50344cf5","indexName":"bloghu","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":200,"position":"top","messagePrev":"这篇博文已经是","messageNext":"天前更新的了, 文章中的内容很可能过时了。"},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '计算智能课程设计(以人事招聘为例的误差反向传播算法)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-02 13:06:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/font.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/you.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">90</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">58</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">胡小宁的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">计算智能课程设计(以人事招聘为例的误差反向传播算法)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-11-30T00:19:23.000Z" title="发表于 2021-11-30 08:19:23">2021-11-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-02T05:06:52.878Z" title="更新于 2021-12-02 13:06:52">2021-12-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="计算智能课程设计(以人事招聘为例的误差反向传播算法)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>昨天写了基于感知机的鸢尾花分类，今天下午要考《大数据技术原理》，本来是整个白天的课程设计，因为考试少了一下午。之前看书复习了一波，但记忆是需要反复锤炼的，所以要抓紧写完这个传播算法，抽出时间再去复习会。2021.11.30/2021.12.1/2021.12.2</p>
<h1 id="以人事招聘为例的误差反向传播算法"><a href="#以人事招聘为例的误差反向传播算法" class="headerlink" title="以人事招聘为例的误差反向传播算法"></a>以人事招聘为例的误差反向传播算法</h1><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><p>理解多层神经网络的结构和原理，掌握反向传播算法对神经元的训练过程，了解反向传播公式。通过构建 BP 网络实例，熟悉前馈网络的原理及结构。</p>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p>误差反向传播算法即 BP 算法，是一种适合于多层神经网络的学习算法。其建立在梯度下降方法的基础之上,主要由激励传播和权重更新两个环节组成，经过反复迭代更新、修正权值从而输出预期的结果。BP 算法整体上可以分成正向传播和反向传播，原理如下:正向传播过程：信息经过输入层到达隐含层，再经过多个隐含层的处理后到达输出层。反向传播过程：比较输出结果和正确结果，将误差作为一个目标函数进行反向传播：对每一层依次求对权值的偏导数，构成目标函数对权值的梯度，网络权重再依次完成更新调整。依此往复、直到输出达到目标值完成训练。该算法可以总结为:利用输出误差推算前一层的误差，再用推算误差算出更前一层的误差，直到计算出所有层的误差估计。1986 年，Hinton 在论文《Learning Representations by Back-propagating Errors》中首次系统地描述了如何利用 BP 算法来训练神经网络。从此，BP 算法开始占据有监督神经网络算法的核心地位。它是迄今最成功的神经网络学习算法之一，现实任务中使用神经网络时，大多是在使用 BP 算法进行训练。</p>
<p>为了说明 BP 算法的过程，本实验使用一个公司招聘的例子：假设有一个公司，其人员招聘由 5个人组成的人事管理部门负责，如下图所示：</p>
<p><img src="/images/python/image-20211130082435748.png" alt="图层"></p>
<p>其中张三、李四等人是应聘者，他们向该部门投递简历，简历包括两类数据：学习成绩和社会实践得分，人事部门有三个层级，一科长根据应聘者的学习成绩和实践得分评估其智商，二科长根据同样的资料评估其情商；一处长根据两个科长提供的智商、情商评分，评估应聘者的工作能力，二处长评估工作态度；最后由总裁汇总两位处长的意见，得出最终结论，即是否招收该应聘者。该模型等价于一个形状为(2,2,2,1)的前馈神经网络，输入层、隐藏层 1、隐藏层 2、输出层各自包含 2、2、2、1 个节点，如下图所示。</p>
<p><img src="/images/python/image-20211130082658780.png" alt="图层"></p>
<p><img src="../images/python/image-20211130082805365.png" alt="image-20211130082805365"></p>
<p><img src="/images/python/image-20211130082833857.png" alt="图层"></p>
<h2 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 输入数据1行2列，这里只有张三的数据</span><br>X = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">0.1</span>]])<br><span class="hljs-comment"># X = np.array([[1,0.1],</span><br><span class="hljs-comment">#               [0.1,1],</span><br><span class="hljs-comment">#               [0.1,0.1],</span><br><span class="hljs-comment">#               [1,1]])</span><br><span class="hljs-comment"># 标签，也叫真值，1行1列，张三的真值：一定录用</span><br>T = np.array([[<span class="hljs-number">1</span>]])<br><span class="hljs-comment"># T = np.array([[1],</span><br><span class="hljs-comment">#               [0],</span><br><span class="hljs-comment">#               [0],</span><br><span class="hljs-comment">#               [1]])</span><br><br><span class="hljs-comment"># 定义一个2隐层的神经网络：2-2-2-1</span><br><span class="hljs-comment"># 输入层2个神经元，隐藏1层2个神经元，隐藏2层2个神经元，输出层1个神经元</span><br><br><span class="hljs-comment"># 输入层到隐藏层1的权值初始化，2行2列</span><br>W1 = np.array([[<span class="hljs-number">0.8</span>,<span class="hljs-number">0.2</span>],<br>              [<span class="hljs-number">0.2</span>,<span class="hljs-number">0.8</span>]])<br><span class="hljs-comment"># 隐藏层1到隐藏层2的权值初始化，2行2列</span><br>W2 = np.array([[<span class="hljs-number">0.5</span>,<span class="hljs-number">0.0</span>],<br>              [<span class="hljs-number">0.5</span>,<span class="hljs-number">1.0</span>]])<br><span class="hljs-comment"># 隐藏层2到输出层的权值初始化，2行1列</span><br>W3 = np.array([[<span class="hljs-number">0.5</span>],<br>              [<span class="hljs-number">0.5</span>]])<br><br><br><span class="hljs-comment"># 初始化偏置值</span><br><span class="hljs-comment"># 隐藏层1的2个神经元偏置</span><br>b1 = np.array([[-<span class="hljs-number">1</span>,<span class="hljs-number">0.3</span>]])<br><span class="hljs-comment"># 隐藏层2的2个神经元偏置</span><br>b2 = np.array([[<span class="hljs-number">0.1</span>,-<span class="hljs-number">0.1</span>]])<br><span class="hljs-comment"># 输出层的1个神经元偏置</span><br>b3 = np.array([[-<span class="hljs-number">0.6</span>]])<br><span class="hljs-comment"># 学习率设置</span><br>lr = <span class="hljs-number">0.1</span><br><span class="hljs-comment"># 定义训练周期数10000</span><br>epochs = <span class="hljs-number">10000</span><br><span class="hljs-comment"># 每训练1000次计算一次loss值  # 定义测试周期数</span><br>report = <span class="hljs-number">1000</span><br><span class="hljs-comment"># 将所有样本分组，每组大小为</span><br>batch_size = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># 定义sigmoid函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br><span class="hljs-comment"># 定义sigmoid函数导数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dsigmoid</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> x*(<span class="hljs-number">1</span>-x)<br><br><span class="hljs-comment"># 更新权值和偏置值</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span>():</span><br>    <span class="hljs-keyword">global</span> batch_X,batch_T,W1,W2,W3,lr,b1,b2,b3<br>    <br>    <span class="hljs-comment"># 隐藏层1输出</span><br>    Z1 = np.dot(batch_X,W1) + b1    <br>    A1 = sigmoid(Z1)<br><br>    <span class="hljs-comment"># 隐藏层2输出</span><br>    Z2 = (np.dot(A1,W2) + b2)<br>    A2 = sigmoid(Z2)<br>    <br>    <span class="hljs-comment"># 输出层输出</span><br>    Z3=(np.dot(A2,W3) + b3)<br>    A3 = sigmoid(Z3)<br>    <br>    <span class="hljs-comment"># 求输出层的误差</span><br>    delta_A3 = (batch_T - A3)<br>    delta_Z3 = delta_A3 * dsigmoid(A3)<br>    <br>    <span class="hljs-comment"># 利用输出层的误差，求出三个偏导（即隐藏层2到输出层的权值改变）    # 由于一次计算了多个样本，所以需要求平均</span><br>    delta_W3 = A2.T.dot(delta_Z3) / batch_X.shape[<span class="hljs-number">0</span>]<br>    delta_B3 = np.<span class="hljs-built_in">sum</span>(delta_Z3, axis=<span class="hljs-number">0</span>) / batch_X.shape[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># 求隐藏层2的误差</span><br>    delta_A2 = delta_Z3.dot(W3.T)<br>    delta_Z2 = delta_A2 * dsigmoid(A2)<br>    <br>    <span class="hljs-comment"># 利用隐藏层2的误差，求出三个偏导（即隐藏层1到隐藏层2的权值改变）    # 由于一次计算了多个样本，所以需要求平均</span><br>    delta_W2 = A1.T.dot(delta_Z2) / batch_X.shape[<span class="hljs-number">0</span>]<br>    delta_B2 = np.<span class="hljs-built_in">sum</span>(delta_Z2, axis=<span class="hljs-number">0</span>) / batch_X.shape[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># 求隐藏层1的误差</span><br>    delta_A1 = delta_Z2.dot(W2.T)<br>    delta_Z1 = delta_A1 * dsigmoid(A1)<br>    <br>    <span class="hljs-comment"># 利用隐藏层1的误差，求出三个偏导（即输入层到隐藏层1的权值改变）    # 由于一次计算了多个样本，所以需要求平均</span><br>    delta_W1 = batch_X.T.dot(delta_Z1) / batch_X.shape[<span class="hljs-number">0</span>]<br>    delta_B1 = np.<span class="hljs-built_in">sum</span>(delta_Z1, axis=<span class="hljs-number">0</span>) / batch_X.shape[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># 更新权值</span><br>    W3 = W3 + lr *delta_W3<br>    W2 = W2 + lr *delta_W2<br>    W1 = W1 + lr *delta_W1<br>    <br>    <span class="hljs-comment"># 改变偏置值</span><br>    b3 = b3 + lr * delta_B3<br>    b2 = b2 + lr * delta_B2<br>    b1 = b1 + lr * delta_B1<br><br><span class="hljs-comment"># 定义空list用于保存loss</span><br>loss = []<br>batch_X = []<br>batch_T = []<br>max_batch = X.shape[<span class="hljs-number">0</span>] // batch_size<br><span class="hljs-comment"># 训练模型</span><br><span class="hljs-keyword">for</span> idx_epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <br>    <span class="hljs-keyword">for</span> idx_batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_batch):<br>        <span class="hljs-comment"># 更新权值</span><br>        batch_X = X[idx_batch*batch_size:(idx_batch+<span class="hljs-number">1</span>)*batch_size, :]<br>        batch_T = T[idx_batch*batch_size:(idx_batch+<span class="hljs-number">1</span>)*batch_size, :]<br>        update()<br>    <span class="hljs-comment"># 每训练5000次计算一次loss值</span><br>    <span class="hljs-keyword">if</span> idx_epoch % report == <span class="hljs-number">0</span>:<br>        <span class="hljs-comment"># 隐藏层1输出</span><br>        A1 = sigmoid(np.dot(X,W1) + b1)<br>        <span class="hljs-comment"># 隐藏层2输出</span><br>        A2 = sigmoid(np.dot(A1,W2) + b2)<br>        <span class="hljs-comment"># 输出层输出</span><br>        A3 = sigmoid(np.dot(A2,W3) + b3)<br>        <span class="hljs-comment"># 计算loss值</span><br>        print(<span class="hljs-string">&#x27;A3:&#x27;</span>,A3)<br>        print(<span class="hljs-string">&#x27;epochs:&#x27;</span>,idx_epoch,<span class="hljs-string">&#x27;loss:&#x27;</span>,np.mean(np.square(T - A3) / <span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># 保存loss值</span><br>        loss.append(np.mean(np.square(T - A3) / <span class="hljs-number">2</span>))<br><br><span class="hljs-comment"># 画图训练周期数与loss的关系图</span><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,epochs,report),loss)<br>plt.xlabel(<span class="hljs-string">&#x27;epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;loss&#x27;</span>)<br>plt.show()<br>        <br><span class="hljs-comment"># 隐藏层1输出</span><br>A1 = sigmoid(np.dot(X,W1) + b1)<br><span class="hljs-comment"># 隐藏层2输出</span><br>A2 = sigmoid(np.dot(A1,W2) + b2)<br><span class="hljs-comment"># 输出层输出</span><br>A3 = sigmoid(np.dot(A2,W3) + b3)<br>print(<span class="hljs-string">&#x27;output:&#x27;</span>)<br>print(A3)<br><br><span class="hljs-comment"># 因为最终的分类只有0和1，所以我们可以把</span><br><span class="hljs-comment"># 大于等于0.5的值归为1类，小于0.5的值归为0类</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">if</span> x&gt;=<span class="hljs-number">0.5</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br><span class="hljs-comment"># map会根据提供的函数对指定序列做映射</span><br><span class="hljs-comment"># 相当于依次把A2中的值放到predict函数中计算</span><br><span class="hljs-comment"># 然后打印出结果</span><br>print(<span class="hljs-string">&#x27;predict:&#x27;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">map</span>(predict,A3):<br>    print(i)<br></code></pre></td></tr></table></figure>
<h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><p>请回答下列问题：</p>
<h3 id="第一题"><a href="#第一题" class="headerlink" title="第一题"></a>第一题</h3><p><strong>1. 如果去掉总裁这一层，相应张三的样本修改为(1.0,0.1,1.0,1.0)，分别对应张三的学习成绩、张三的实践成绩、张三的工作能力真值、张三的工作态度真值，代码应该如何修改？</strong></p>
<p>要修改的代码：</p>
<ul>
<li><p>更改的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">       <span class="hljs-comment"># 求隐藏层2的误差</span><br><span class="hljs-number">80.</span>    delta_A2 = delta_Z3.dot(batch_T - A2)<br><br>       <span class="hljs-comment"># 计算loss值</span><br><span class="hljs-number">127.</span>    print(<span class="hljs-string">&#x27;A2:&#x27;</span>,A2)<br><span class="hljs-number">128.</span>    print(<span class="hljs-string">&#x27;epochs:&#x27;</span>,idx_epoch,<span class="hljs-string">&#x27;loss:&#x27;</span>,np.mean(np.square(T - A2) / <span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># 保存loss值</span><br><span class="hljs-number">130.</span>    loss.append(np.mean(np.square(T - A2) / <span class="hljs-number">2</span>))<br><br><span class="hljs-number">145.</span>    print(A3)<br><br><span class="hljs-number">159.</span>      <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">map</span>(predict,A2.T):<br></code></pre></td></tr></table></figure>


</li>
</ul>
<ul>
<li><p>删除的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python">       <span class="hljs-comment"># 隐藏层2到输出层的权值初始化，2行1列</span><br><span class="hljs-number">27.</span>	   W3 = np.array([[<span class="hljs-number">0.5</span>],<br><span class="hljs-number">28.</span>              [<span class="hljs-number">0.5</span>]])<br>       <span class="hljs-comment"># 输出层的1个神经元偏置</span><br><span class="hljs-number">37.</span>	   b3 = np.array([[-<span class="hljs-number">0.6</span>]])<br><br>       <span class="hljs-comment"># 输出层输出</span><br><span class="hljs-number">68.</span>    Z3=(np.dot(A2,W3) + b3)<br><span class="hljs-number">69.</span>    A3 = sigmoid(Z3)<br><br>       <span class="hljs-comment"># 求输出层的误差</span><br><span class="hljs-number">72.</span>    delta_A3 = (batch_T - A3)<br><span class="hljs-number">73.</span>    delta_Z3 = delta_A3 * dsigmoid(A3)<br><br>       <span class="hljs-comment"># 利用输出层的误差，求出三个偏导（即隐藏层2到输出层的权值改变）    # 由于一次计算了多个样本，所以需要求平均</span><br><span class="hljs-number">76.</span>    delta_W3 = A2.T.dot(delta_Z3) / batch_X.shape[<span class="hljs-number">0</span>]<br><span class="hljs-number">77.</span>    delta_B3 = np.<span class="hljs-built_in">sum</span>(delta_Z3, axis=<span class="hljs-number">0</span>) / batch_X.shape[<span class="hljs-number">0</span>]<br>      <span class="hljs-comment"># 更新权值</span><br><span class="hljs-number">96.</span>       W3 = W3 + lr *delta_W3<br>       <span class="hljs-comment"># 改变偏置值</span><br><span class="hljs-number">101.</span>      b3 = b3 + lr * delta_B3<br>       <span class="hljs-comment"># 输出层输出</span><br><span class="hljs-number">125.</span>      A3 = sigmoid(np.dot(A2,W3) + b3)<br>       <span class="hljs-comment"># 输出层输出</span><br><span class="hljs-number">143.</span>      A3 = sigmoid(np.dot(A2,W3) + b3)<br></code></pre></td></tr></table></figure>
<p>结果：</p>
</li>
</ul>
<p><img src="/images/python/image-20211201185825643.png" alt="结果"></p>
<p><img src="/images/python/image-20211201185842303.png" alt="结果"></p>
<h3 id="第二题"><a href="#第二题" class="headerlink" title="第二题"></a>第二题</h3><p><strong>2..如果增加一个样本，李四（0.1,1.0,0），分别对应李四的学习成绩，李四的实践成绩，李四被招聘可能性的真值，代码应该如何修改？此时是一个样本计算一次偏导、更新一次权值，还是两个样本一起计算一次偏导、更新一次权值？（提示：注意 batch_size 的作用）</strong></p>
<p>修改的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">5.</span>  X = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">0.1</span>],<br>              [<span class="hljs-number">0.1</span>,<span class="hljs-number">1</span>]])<br><br><span class="hljs-number">11.</span> T = np.array([[<span class="hljs-number">1</span>],<br>                 [<span class="hljs-number">0</span>]])<br><br></code></pre></td></tr></table></figure>
<p>结果：</p>
<p><img src="/images/python/image-20211201201654012.png" alt="增加李四后的结果"></p>
<p><img src="/images/python/image-20211201201741748.png" alt="loss-epoch"></p>
<p>此时，我们通过修改代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">114.</span>	<span class="hljs-keyword">for</span> idx_batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_batch):<br>        <span class="hljs-comment"># 更新权值</span><br>        	batch_X = X[idx_batch*batch_size:(idx_batch+<span class="hljs-number">1</span>)*batch_size, :]<br>        	batch_T = T[idx_batch*batch_size:(idx_batch+<span class="hljs-number">1</span>)*batch_size, :]<br>            <span class="hljs-comment"># 以下为新增的代码</span><br>        	print(batch_X)<br>        	print(batch_T)<br>            <span class="hljs-comment"># 以上为新增的代码</span><br>        	update()<br></code></pre></td></tr></table></figure>
<p>运行程序不难看出：</p>
<p><img src="/images/python/image-20211202081112178.png" alt="程序结果"></p>
<p>当batch_size==1时，一个样本计算一次偏导，更新一次权值。</p>
<h3 id="第三题"><a href="#第三题" class="headerlink" title="第三题"></a>第三题</h3><p><strong>3.样本为张三[1,0.1,1]、李四[0.1,1,0]、王五[0.1,0.1,0]、赵六[1,1,1]，请利用 batch_size 实现教材 279 页提到的“批量梯度下降”、“随机梯度下降”和“小批量梯度下降”，请注意“随机梯度下降”和“小批量梯度下降”要体现随机性。</strong></p>
<h4 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h4><p>修改代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">4.</span>	<span class="hljs-comment"># 输入数据4行2列，这里有张三、李四、王五、赵六的数据</span><br>	X = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">0.1</span>],<br>                  [<span class="hljs-number">0.1</span>,<span class="hljs-number">1</span>],<br>                  [<span class="hljs-number">0.1</span>,<span class="hljs-number">0.1</span>],<br>                  [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]])<br>	<span class="hljs-comment"># 标签，也叫真值，4行1列，张三的真值：一定录用，李四的真值：一定不录用</span><br>	T = np.array([[<span class="hljs-number">1</span>],<br>              	[<span class="hljs-number">0</span>],<br>              	[<span class="hljs-number">0</span>],<br>              	[<span class="hljs-number">1</span>]])<br>    <br>    <span class="hljs-comment"># batch_size</span><br>     batch_size = <span class="hljs-number">4</span><br></code></pre></td></tr></table></figure>
<p>此时，四个样本一起计算一次偏导，更新一次权值，进行一次迭代。符合在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新。</p>
<p>随机梯度下降训练结果：</p>
<p><img src="/images/python/image-20211202085850278.png" alt="随机梯度下降训练结果"></p>
<p><img src="/images/python/image-20211202085910390.png" alt="随机梯度下降训练结果"></p>
<h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><p>修改代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">     <span class="hljs-comment"># 将所有样本分组，每组大小为1</span><br>batch_size = <span class="hljs-number">1</span><br>    <br><span class="hljs-number">114.</span> <span class="hljs-keyword">for</span> idx_batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_batch):<br>      <span class="hljs-comment"># 更新权值</span><br>      <span class="hljs-comment"># 随机样本</span><br>      idx_batch = random.randint(<span class="hljs-number">0</span>,max_batch-<span class="hljs-number">1</span>)<br>      batch_X = X[(idx_batch)*batch_size:(idx_batch+<span class="hljs-number">1</span>)*batch_size, :]<br>      batch_T = T[(idx_batch)*batch_size:(idx_batch+<span class="hljs-number">1</span>)*batch_size, :]<br>      update()<br></code></pre></td></tr></table></figure>
<p>随机性体现如下：</p>
<p><img src="/images/python/image-20211202093137751.png" alt="随机性"></p>
<p>随机批量梯度下降法结果：</p>
<p><img src="/images/python/image-20211202093205136.png" alt="随机批量梯度下降法结果"></p>
<p><img src="/images/python/image-20211202093220168.png" alt="随机批量梯度下降法结果"></p>
<h4 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h4><p><strong>在这里，我们设置小批量梯度下降的m为2.</strong></p>
<p>修改代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将所有样本分组，每组大小为</span><br> batch_size = <span class="hljs-number">2</span><br><br> <span class="hljs-keyword">for</span> idx_batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_batch):<br>        <span class="hljs-comment"># 更新权值</span><br>        <span class="hljs-comment"># 随机样本</span><br>        r_nums = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">1.5</span>,<span class="hljs-number">2</span>])<br>        r_num = random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>)<br>        batch_X = X[<span class="hljs-built_in">int</span>((r_nums[r_num]-<span class="hljs-number">1</span>))*batch_size:(<span class="hljs-built_in">int</span>(r_nums[r_num]))*batch_size]<br>        batch_T = T[<span class="hljs-built_in">int</span>((r_nums[r_num]-<span class="hljs-number">1</span>))*batch_size:(<span class="hljs-built_in">int</span>(r_nums[r_num]))*batch_size]<br>        update()<br><br>随机性体现如下：<br></code></pre></td></tr></table></figure>
<p><img src="/images/python/image-20211202101330163.png" alt="随机性体现"></p>
<p>小批量随机梯度下降结果：</p>
<p><img src="/images/python/image-20211202101532482.png" alt="小批量随机梯度下降"></p>
<p><img src="/images/python/image-20211202101645645.png" alt="小批量随机梯度下降"></p>
<h3 id="第四题"><a href="#第四题" class="headerlink" title="第四题"></a>第四题</h3><p><strong>4.【 选 做 】 本 例 中 输 入 向 量 、 真 值 都 是 行 向 量 ， 请 将 它 们 修 改 为 列 向 量 ， 如X = np.array([[1,0.1]])改为 X = np.array([[1],[0.1]])，请合理修改其它部分以使程序得到与行向量时相同的结果。</strong></p>
<p>基于第三题<strong>小批量随机梯度</strong>下降更改的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 输入数据2行4列</span><br><span class="hljs-comment"># X = np.array([[1,0.1],[0.1,1],[0.1,0.1],[1,1]])</span><br>X = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">0.1</span>,<span class="hljs-number">0.1</span>,<span class="hljs-number">1</span>],<br>             [<span class="hljs-number">0.1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0.1</span>,<span class="hljs-number">1</span>]])<br><span class="hljs-comment"># X = np.array([[1,0.1],</span><br><span class="hljs-comment">#               [0.1,1],</span><br><span class="hljs-comment">#               [0.1,0.1],</span><br><span class="hljs-comment">#               [1,1]])</span><br><span class="hljs-comment"># 标签，也叫真值，1行4列，张三的真值：一定录用</span><br>T = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]])<br><span class="hljs-comment"># T = np.array([[1],</span><br><span class="hljs-comment">#               [0],</span><br><span class="hljs-comment">#               [0],</span><br><span class="hljs-comment">#               [1]])</span><br><br><span class="hljs-comment"># 定义一个2隐层的神经网络：2-2-2-1</span><br><span class="hljs-comment"># 输入层2个神经元，隐藏1层2个神经元，隐藏2层2个神经元，输出层1个神经元</span><br><br><span class="hljs-comment"># 输入层到隐藏层1的权值初始化，2行2列</span><br>W1 = np.array([[<span class="hljs-number">0.8</span>,<span class="hljs-number">0.2</span>],<br>              [<span class="hljs-number">0.2</span>,<span class="hljs-number">0.8</span>]])<br><span class="hljs-comment"># 隐藏层1到隐藏层2的权值初始化，2行2列</span><br>W2 = np.array([[<span class="hljs-number">0.5</span>,<span class="hljs-number">0.0</span>],<br>              [<span class="hljs-number">0.5</span>,<span class="hljs-number">1.0</span>]])<br><span class="hljs-comment"># 隐藏层2到输出层的权值初始化，2行1列</span><br>W3 = np.array([[<span class="hljs-number">0.5</span>],<br>              [<span class="hljs-number">0.5</span>]])<br><br><br><span class="hljs-comment"># 初始化偏置值</span><br><span class="hljs-comment"># 隐藏层1的2个神经元偏置</span><br>b1 = np.array([[-<span class="hljs-number">1</span>,<span class="hljs-number">0.3</span>]])<br><span class="hljs-comment"># 隐藏层2的2个神经元偏置</span><br>b2 = np.array([[<span class="hljs-number">0.1</span>,-<span class="hljs-number">0.1</span>]])<br><span class="hljs-comment"># 输出层的1个神经元偏置</span><br>b3 = np.array([[-<span class="hljs-number">0.6</span>]])<br><span class="hljs-comment"># 学习率设置</span><br>lr = <span class="hljs-number">0.1</span><br><span class="hljs-comment"># 定义训练周期数10000</span><br>epochs = <span class="hljs-number">10000</span><br><span class="hljs-comment"># 每训练1000次计算一次loss值  # 定义测试周期数</span><br>report = <span class="hljs-number">1000</span><br><span class="hljs-comment"># 将所有样本分组，每组大小为</span><br>batch_size = <span class="hljs-number">2</span><br><br><span class="hljs-comment"># 定义sigmoid函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br><span class="hljs-comment"># 定义sigmoid函数导数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dsigmoid</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> x*(<span class="hljs-number">1</span>-x)<br><br><span class="hljs-comment"># 更新权值和偏置值</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span>():</span><br>    <span class="hljs-keyword">global</span> batch_X,batch_T,W1,W2,W3,lr,b1,b2,b3<br>    <br>    <span class="hljs-comment"># 隐藏层1输出</span><br>    Z1 = np.dot(batch_X,W1) + b1    <br>    A1 = sigmoid(Z1)<br><br>    <span class="hljs-comment"># 隐藏层2输出</span><br>    Z2 = (np.dot(A1,W2) + b2)<br>    A2 = sigmoid(Z2)<br><br>    <span class="hljs-comment"># 输出层输出</span><br>    Z3=(np.dot(A2,W3) + b3)<br>    A3 = sigmoid(Z3)<br><br>    <span class="hljs-comment"># 求输出层的误差</span><br>    delta_A3 = (batch_T - A3)<br>    delta_Z3 = delta_A3 * dsigmoid(A3)<br><br>    <span class="hljs-comment"># 利用输出层的误差，求出三个偏导（即隐藏层2到输出层的权值改变）    # 由于一次计算了多个样本，所以需要求平均</span><br>    delta_W3 = A2.T.dot(delta_Z3) / batch_X.shape[<span class="hljs-number">0</span>]<br>    delta_B3 = np.<span class="hljs-built_in">sum</span>(delta_Z3, axis=<span class="hljs-number">0</span>) / batch_X.shape[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># 求隐藏层2的误差</span><br>    delta_A2 = delta_Z3.dot(W3.T)<br>    delta_Z2 = delta_A2 * dsigmoid(A2)<br>    <br>    <span class="hljs-comment"># 利用隐藏层2的误差，求出三个偏导（即隐藏层1到隐藏层2的权值改变）    # 由于一次计算了多个样本，所以需要求平均</span><br>    delta_W2 = A1.T.dot(delta_Z2) / batch_X.shape[<span class="hljs-number">0</span>]<br>    delta_B2 = np.<span class="hljs-built_in">sum</span>(delta_Z2, axis=<span class="hljs-number">0</span>) / batch_X.shape[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># 求隐藏层1的误差</span><br>    delta_A1 = delta_Z2.dot(W2.T)<br>    delta_Z1 = delta_A1 * dsigmoid(A1)<br>    <br>    <span class="hljs-comment"># 利用隐藏层1的误差，求出三个偏导（即输入层到隐藏层1的权值改变）    # 由于一次计算了多个样本，所以需要求平均</span><br>    delta_W1 = batch_X.T.dot(delta_Z1) / batch_X.shape[<span class="hljs-number">0</span>]<br>    delta_B1 = np.<span class="hljs-built_in">sum</span>(delta_Z1, axis=<span class="hljs-number">0</span>) / batch_X.shape[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># 更新权值</span><br>    W3 = W3 + lr *delta_W3<br>    W2 = W2 + lr *delta_W2<br>    W1 = W1 + lr *delta_W1<br>    <br>    <span class="hljs-comment"># 改变偏置值</span><br>    b3 = b3 + lr * delta_B3<br>    b2 = b2 + lr * delta_B2<br>    b1 = b1 + lr * delta_B1<br><br><span class="hljs-comment"># 定义空list用于保存loss</span><br>loss = []<br>batch_X = []<br>batch_T = []<br>max_batch = X.shape[<span class="hljs-number">1</span>] // batch_size<br><span class="hljs-comment"># 训练模型</span><br><span class="hljs-keyword">for</span> idx_epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <br>    <span class="hljs-keyword">for</span> idx_batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_batch):<br>        <span class="hljs-comment"># 更新权值</span><br>        <span class="hljs-comment"># 随机样本</span><br>        r_nums = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">1.5</span>,<span class="hljs-number">2</span>])<br>        r_num = random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>)<br>        batch_X = X.T[<span class="hljs-built_in">int</span>((r_nums[r_num]-<span class="hljs-number">1</span>))*batch_size:(<span class="hljs-built_in">int</span>(r_nums[r_num]))*batch_size]<br>        batch_T = T.T[<span class="hljs-built_in">int</span>((r_nums[r_num]-<span class="hljs-number">1</span>))*batch_size:(<span class="hljs-built_in">int</span>(r_nums[r_num]))*batch_size]<br>        print(batch_X)<br>        print(batch_T)<br>        update()<br>    <span class="hljs-comment"># 每训练5000次计算一次loss值</span><br>    <span class="hljs-keyword">if</span> idx_epoch % report == <span class="hljs-number">0</span>:<br>        <span class="hljs-comment"># 隐藏层1输出</span><br>        A1 = sigmoid(np.dot(X.T,W1) + b1)<br>        <span class="hljs-comment"># 隐藏层2输出</span><br>        A2 = sigmoid(np.dot(A1,W2) + b2)<br>        <span class="hljs-comment"># 输出层输出</span><br>        A3 = sigmoid(np.dot(A2,W3) + b3)<br>        <span class="hljs-comment"># 计算loss值</span><br>        print(<span class="hljs-string">&#x27;A3:&#x27;</span>,A3)<br>        print(<span class="hljs-string">&#x27;epochs:&#x27;</span>,idx_epoch,<span class="hljs-string">&#x27;loss:&#x27;</span>,np.mean(np.square(T.T - A3) / <span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># 保存loss值</span><br>        loss.append(np.mean(np.square(T.T - A3) / <span class="hljs-number">2</span>))<br><br><span class="hljs-comment"># 画图训练周期数与loss的关系图</span><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,epochs,report),loss)<br>plt.xlabel(<span class="hljs-string">&#x27;epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;loss&#x27;</span>)<br>plt.show()<br>        <br><span class="hljs-comment"># 隐藏层1输出</span><br>A1 = sigmoid(np.dot(X.T,W1) + b1)<br><span class="hljs-comment"># 隐藏层2输出</span><br>A2 = sigmoid(np.dot(A1,W2) + b2)<br><span class="hljs-comment"># 输出层输出</span><br>A3 = sigmoid(np.dot(A2,W3) + b3)<br>print(<span class="hljs-string">&#x27;output:&#x27;</span>)<br>print(A3)<br><br><br><span class="hljs-comment"># 因为最终的分类只有0和1，所以我们可以把</span><br><span class="hljs-comment"># 大于等于0.5的值归为1类，小于0.5的值归为0类</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">if</span> x&gt;=<span class="hljs-number">0.5</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br><span class="hljs-comment"># map会根据提供的函数对指定序列做映射</span><br><span class="hljs-comment"># 相当于依次把A2中的值放到predict函数中计算</span><br><span class="hljs-comment"># 然后打印出结果</span><br>print(<span class="hljs-string">&#x27;predict:&#x27;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">map</span>(predict,A3):<br>    print(i)<br></code></pre></td></tr></table></figure>
<p>运行效果：</p>
<p><img src="/images/python/image-20211202105017115.png" alt="效果"></p>
<p><img src="/images/python/image-20211202105031415.png" alt="效果"></p>
<h2 id="实验涉及到的实验语法知识"><a href="#实验涉及到的实验语法知识" class="headerlink" title="实验涉及到的实验语法知识"></a>实验涉及到的实验语法知识</h2><h3 id="numpy-dot-函数的用法-1"><a href="#numpy-dot-函数的用法-1" class="headerlink" title="numpy.dot()函数的用法 [1]"></a>numpy.dot()函数的用法 [1]</h3><p>numpy.<strong>dot</strong>(<em>a</em>, <em>b</em>, <em>out=None</em>)</p>
<p>Dot product of two arrays. Specifically,</p>
<ul>
<li><p>If both <em>a</em> and <em>b</em> are 1-D arrays, it is inner product of vectors (without complex conjugation).</p>
</li>
<li><p>If both <em>a</em> and <em>b</em> are 2-D arrays, it is matrix multiplication, but using <a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference/generated/numpy.matmul.html#numpy.matmul"><code>matmul</code></a> or <code>a @ b</code> is preferred.</p>
</li>
<li><p>If either <em>a</em> or <em>b</em> is 0-D (scalar), it is equivalent to <a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference/generated/numpy.multiply.html#numpy.multiply"><code>multiply</code></a> and using <code>numpy.multiply(a, b)</code> or <code>a * b</code> is preferred.</p>
</li>
<li><p>If <em>a</em> is an N-D array and <em>b</em> is a 1-D array, it is a sum product over the last axis of <em>a</em> and <em>b</em>.</p>
</li>
<li><p>If <em>a</em> is an N-D array and <em>b</em> is an M-D array (where <code>M&gt;=2</code>), it is a sum product over the last axis of <em>a</em> and the second-to-last axis of <em>b</em>:</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">dot<span class="hljs-comment">(a, b)</span>[i,j,k,m] = sum<span class="hljs-comment">(a[i,j,:] * b[k,:,m])</span><br></code></pre></td></tr></table></figure>
<p>Examples</p>
</li>
</ul>
<figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta">&gt;&gt;&gt;</span> <span class="python">np.dot(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)</span><br>12<br></code></pre></td></tr></table></figure>
<p>Neither argument is complex-conjugated:</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css">&gt;&gt;&gt; <span class="hljs-selector-tag">np</span><span class="hljs-selector-class">.dot</span>(<span class="hljs-selector-attr">[2j, 3j]</span>, <span class="hljs-selector-attr">[2j, 3j]</span>)<br>(<span class="hljs-selector-tag">-13</span>+0<span class="hljs-selector-tag">j</span>)<br></code></pre></td></tr></table></figure>
<p>For 2-D arrays it is the matrix product:</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs lua">&gt;&gt;&gt; a = <span class="hljs-string">[[1, 0], [0, 1]]</span><br>&gt;&gt;&gt; b = <span class="hljs-string">[[4, 1], [2, 2]]</span><br>&gt;&gt;&gt; np.dot(a, b)<br>array(<span class="hljs-string">[[4, 1],</span><br><span class="hljs-string">       [2, 2]]</span>)<br>&gt;&gt;&gt; a = np.arange(<span class="hljs-number">3</span>*<span class="hljs-number">4</span>*<span class="hljs-number">5</span>*<span class="hljs-number">6</span>).reshape((<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>))<br>&gt;&gt;&gt; b = np.arange(<span class="hljs-number">3</span>*<span class="hljs-number">4</span>*<span class="hljs-number">5</span>*<span class="hljs-number">6</span>)[::<span class="hljs-number">-1</span>].reshape((<span class="hljs-number">5</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>,<span class="hljs-number">3</span>))<br>&gt;&gt;&gt; np.dot(a, b)[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]<br><span class="hljs-number">499128</span><br>&gt;&gt;&gt; sum(a[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,:] * b[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,:,<span class="hljs-number">2</span>])<br><span class="hljs-number">499128</span><br></code></pre></td></tr></table></figure>
<p>总结：</p>
<ol>
<li>如果处理的是一维数组，则得到的是两数组的內积</li>
<li>如果是二维数组（矩阵）之间的运算，则得到的是矩阵积（mastrix product）</li>
</ol>
<p><img src="http://images2017.cnblogs.com/blog/1281542/201711/1281542-20171130030615620-21120532.png" alt="img"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 一维数组的情况</span><br>In : d = np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">9</span>)<br>Out: array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>])<br>In : e = d[::-<span class="hljs-number">1</span>]<br>Out: array([<span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])<br><br>In : np.dot(d,e) <br>Out: <span class="hljs-number">84</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">##矩阵的情况</span><br>In : a = np.arange(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>Out:<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><br>In : b = np.arange(<span class="hljs-number">5</span>,<span class="hljs-number">9</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>Out: array([[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],<br>            [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]])<br><br>In : np.dot(a,b)<br>Out:<br>array([[<span class="hljs-number">19</span>, <span class="hljs-number">22</span>],<br>       [<span class="hljs-number">43</span>, <span class="hljs-number">50</span>]])<br></code></pre></td></tr></table></figure>
<h3 id="sigmoid-函数的用法及意义-2"><a href="#sigmoid-函数的用法及意义-2" class="headerlink" title="sigmoid()函数的用法及意义 [2]"></a>sigmoid()函数的用法及意义 [2]</h3><p>sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xMTk1OTA3MS1hNmE5Yjg2YzI5ZmU3NWJiLnBuZz9pbWFnZU1vZ3IyL2F1dG8tb3JpZW50L3N0cmlwfGltYWdlVmlldzIvMi93LzIyNi9mb3JtYXQvd2VicA?x-oss-process=image/format,png" alt="sigmod函数表达式"></p>
<p>这就是sigmoid函数的表达式，这个函数在伯努利分布上非常好用，现在看看他的图像就清楚</p>
<p> <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xMTk1OTA3MS1lZDRjNjVlNWI1ZmE4MzAwLnBuZz9pbWFnZU1vZ3IyL2F1dG8tb3JpZW50L3N0cmlwfGltYWdlVmlldzIvMi93LzI2OC9mb3JtYXQvd2VicA?x-oss-process=image/format,png" alt="img"></p>
<p>可以看到在趋于正无穷或负无穷时，函数趋近平滑状态，sigmoid函数因为输出范围（0，1），所以二分类的概率常常用这个函数，事实上logistic回归采用这个函数很多教程也说了以下几个优点</p>
<pre><code>1  值域在0和1之间

2  函数具有非常好的对称性

函数对输入超过一定范围就会不敏感</code></pre>
<p>sigmoid的输出在0和1之间，我们在二分类任务中，采用sigmoid的输出的是事件概率，也就是当输出满足满足某一概率条件我们将其划分正类，不同于svm。</p>
<p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> math<br> <br> <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid_function</span>(<span class="hljs-params">z</span>):</span><br>    fz = []<br>    <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> z:<br>        fz.append(<span class="hljs-number">1</span>/(<span class="hljs-number">1</span> + math.exp(-num)))<br>    <span class="hljs-keyword">return</span> fz<br> <br> <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    z = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.01</span>)<br>    fz = sigmoid_function(z)<br>    plt.title(<span class="hljs-string">&#x27;Sigmoid Function&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;z&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;σ(z)&#x27;</span>)<br>    plt.plot(z, fz)<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p>输出函数图像：</p>
<p><img src="https://img-blog.csdnimg.cn/2019112521242832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4ODkwNDEy,size_16,color_FFFFFF,t_70" alt="img"></p>
<h3 id="Numpy中的shape函数的用法详解-3"><a href="#Numpy中的shape函数的用法详解-3" class="headerlink" title="Numpy中的shape函数的用法详解 [3]"></a>Numpy中的shape函数的用法详解 [3]</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p><code>x.shape[0]</code> will give the number of rows in an array. In your case it will give output 10. If you will type <code>x.shape[1]</code>, it will print out the number of columns i.e 1024. </p>
<h4 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h4><h5 id="中文解释"><a href="#中文解释" class="headerlink" title="中文解释"></a>中文解释</h5><p>shape函数的功能是读取矩阵的长度，比如shape[0]就是读取矩阵第一维度的长度,相当于行数。它的输入参数可以是一个整数表示维度，也可以是一个矩阵。shape函数返回的是一个元组，表示数组（矩阵）的维度，例子如下：</p>
<ol>
<li>数组（矩阵）只有一个维度时，shape只有shape[0]，返回的是该一维数组（矩阵）中元素的个数，通俗点说就是返回列数，因为一维数组只有一行，一维情况中array创建的可以看做list（或一维数组）</li>
</ol>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a=np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>a<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>a.shape<br>(<span class="hljs-number">2L</span>,)<br><span class="hljs-meta">&gt;&gt;&gt; </span>a.shape[<span class="hljs-number">0</span>]<br><span class="hljs-number">2L</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>a.shape[<span class="hljs-number">1</span>]<br>Traceback (most recent call last):<br>  File <span class="hljs-string">&quot;&lt;pyshell#63&gt;&quot;</span>, line <span class="hljs-number">1</span>, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    a.shape[<span class="hljs-number">1</span>]<br>IndexError: <span class="hljs-built_in">tuple</span> index out of <span class="hljs-built_in">range</span>   <span class="hljs-comment">#最后报错是因为一维数组只有一个维度，可以用a.shape或a.shape[0]来访问</span><br></code></pre></td></tr></table></figure>
<ol start="2">
<li>数组有两个维度（即行和列）时，和我们的逻辑思维一样，a.shape返回的元组表示该数组的行数与列数，请看下例：</li>
</ol>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a=np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])    <span class="hljs-comment">#注意二维数组要用（）和[]一起包裹起来，键入print a 会得到一个用2个[]包裹的数组（矩阵）</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>a<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>a.shape<br>(<span class="hljs-number">2L</span>, <span class="hljs-number">2L</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>b=np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>b<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>       [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>b.shape<br>(<span class="hljs-number">2L</span>, <span class="hljs-number">3L</span>)<br></code></pre></td></tr></table></figure>
<h5 id="英文解释"><a href="#英文解释" class="headerlink" title="英文解释"></a>英文解释</h5><p><code>x[0].shape</code> will give the Length of 1st row of an array. <code>x.shape[0]</code> will give the number of rows in an array. In your case it will give output 10. If you will type <code>x.shape[1]</code>, it will print out the number of columns i.e 1024. If you would type <code>x.shape[2]</code>, it will give an error, since we are working on a 2-d array and we are out of index. Let me explain you all the uses of ‘shape’ with a simple example by taking a 2-d array of zeros of dimension 3x4.</p>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-comment">#This will create a 2-d array of zeroes of dimensions 3x4</span><br>x = np.zeros((<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br>print(x)<br>[[ <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>]<br>[ <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>]<br>[ <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>]]<br><br><span class="hljs-comment">#This will print the First Row of the 2-d array</span><br>x[<span class="hljs-number">0</span>]<br>array([ <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">0.</span>])<br><br><span class="hljs-comment">#This will Give the Length of 1st row</span><br>x[<span class="hljs-number">0</span>].shape<br>(<span class="hljs-number">4</span>,)<br><br><span class="hljs-comment">#This will Give the Length of 2nd row, verified that length of row is showing same </span><br>x[<span class="hljs-number">1</span>].shape<br>(<span class="hljs-number">4</span>,)<br><br><span class="hljs-comment">#This will give the dimension of 2-d Array </span><br>x.shape<br>(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><br><span class="hljs-comment"># This will give the number of rows is 2-d array </span><br>x.shape[<span class="hljs-number">0</span>]<br><span class="hljs-number">3</span><br><br><span class="hljs-comment"># This will give the number of columns is 2-d array </span><br>x.shape[<span class="hljs-number">1</span>]<br><span class="hljs-number">3</span><br><br><span class="hljs-comment"># This will give the number of columns is 2-d array </span><br>x.shape[<span class="hljs-number">1</span>]<br><span class="hljs-number">4</span><br><br><span class="hljs-comment"># This will give an error as we have a 2-d array and we are asking value for an index </span><br>out of <span class="hljs-built_in">range</span><br>x.shape[<span class="hljs-number">2</span>]<br>---------------------------------------------------------------------------<br>IndexError                                Traceback (most recent call last)<br>&lt;ipython-<span class="hljs-built_in">input</span>-<span class="hljs-number">20</span>-4b202d084bc7&gt; <span class="hljs-keyword">in</span> &lt;module&gt;()<br>----&gt; 1 x.shape[2]<br><br>IndexError: <span class="hljs-built_in">tuple</span> index out of <span class="hljs-built_in">range</span><br></code></pre></td></tr></table></figure>
<h3 id="NumPy-切片和索引-4"><a href="#NumPy-切片和索引-4" class="headerlink" title="NumPy 切片和索引 [4]"></a>NumPy 切片和索引 [4]</h3><p>ndarray对象的内容可以通过索引或切片来访问和修改，与 Python 中 list 的切片操作一样。</p>
<p>ndarray 数组可以基于 0 - n 的下标进行索引，切片对象可以通过内置的 slice 函数，并设置 start, stop 及 step 参数进行，从原数组中切割出一个新数组。</p>
<p>实例:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np  <br><br>a = np.arange(<span class="hljs-number">10</span>) <br><br>s = <span class="hljs-built_in">slice</span>(<span class="hljs-number">2</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2</span>)   <br><br><span class="hljs-comment">#从索引 2 开始到索引 7 停止，间隔为2</span><br><br> <span class="hljs-built_in">print</span> (a[s])<br></code></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json">[<span class="hljs-number">2</span>  <span class="hljs-number">4</span>  <span class="hljs-number">6</span>]<br></code></pre></td></tr></table></figure>
<p>冒号 <strong>:</strong> 的解释：如果只放置一个参数，如 **[2]**，将返回与该索引相对应的单个元素。如果为 **[2:]**，表示从该索引开始以后的所有项都将被提取。如果使用了两个参数，如 **[2:7]**，那么则提取两个索引(不包括停止索引)之间的项。</p>
<p>多维数组同样适用上述索引提取方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br> <br>a = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],<br>              [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>],<br>              [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])<br>print(a)<br><span class="hljs-comment"># 从某个索引处开始切割</span><br>print(<span class="hljs-string">&#x27;从数组索引 a[1:] 处开始切割&#x27;</span>)<br>print(a[<span class="hljs-number">1</span>:])<br></code></pre></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs lua"><span class="hljs-string">[[1 2 3]</span><br><span class="hljs-string"> [3 4 5]</span><br><span class="hljs-string"> [4 5 6]]</span><br>从数组索引 a[<span class="hljs-number">1</span>:] 处开始切割<br><span class="hljs-string">[[3 4 5]</span><br><span class="hljs-string"> [4 5 6]]</span><br></code></pre></td></tr></table></figure>
<h3 id="批量梯度下降-5"><a href="#批量梯度下降-5" class="headerlink" title="批量梯度下降[5]"></a>批量梯度下降[5]</h3><h4 id="批量梯度下降法"><a href="#批量梯度下降法" class="headerlink" title="批量梯度下降法"></a>批量梯度下降法</h4><p><strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新。</p>
<h4 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h4><p><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是在<strong>每次迭代时</strong>使用<strong>一个样本</strong>来对参数进行更新（mini-batch size =1）。</p>
<h4 id="小批量梯度下降法"><a href="#小批量梯度下降法" class="headerlink" title="小批量梯度下降法"></a>小批量梯度下降法</h4><p><strong>什么是小批量梯度下降？</strong>具体的说：在算法的每一步，我们从具有 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 个样本的<strong>训练集（已经打乱样本的顺序）</strong>中<strong>随机抽出</strong>一小批量(mini-batch)样本 <img src="https://www.zhihu.com/equation?tex=X=(x%5E%7B(1)%7D,...,x%5E%7B(m%5E%7B%27%7D)%7D)" alt="[公式]"> 。小批量的数目 <img src="https://www.zhihu.com/equation?tex=m%5E%7B%E2%80%99%7D" alt="[公式]"> 通常是一个相对较小的数（从1到几百）。重要的是，当训练集大小 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 增长时，<img src="https://www.zhihu.com/equation?tex=m%5E%7B%E2%80%99%7D" alt="[公式]">通常是固定的。我们可能在拟合几十亿的样本时，每次更新计算只用到几百个样本。</p>
<p><strong>参考文献</strong>：</p>
<p>[1]    越来越胖的GuanRunwei.简述Sigmoid函数（附Python代码）[G/OL].CSDN,2019(2019-11-25).<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38890412/article/details/103246057">https://blog.csdn.net/qq_38890412/article/details/103246057</a></p>
<p>[2]    付修磊.Numpy中的shape函数的用法详解[G/OL].CSDN,2018(2018-01-17).<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38669138/article/details/79084275">https://blog.csdn.net/qq_38669138/article/details/79084275</a></p>
<p>[3]    Animesh Johri.x.shape[0] vs x[0].shape in NumPy[G/OL].stackoverflow,2018(2018-9-22).<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/48134598/x-shape0-vs-x0-shape-in-numpy">https://stackoverflow.com/questions/48134598/x-shape0-vs-x0-shape-in-numpy</a></p>
<p>[4]    RUNOOB.COM编者.NumPy 切片和索引[G/OL].RUNOOB.COM,2021..<a target="_blank" rel="noopener" href="https://www.runoob.com/numpy/numpy-indexing-and-slicing.html">https://www.runoob.com/numpy/numpy-indexing-and-slicing.html</a></p>
<p>[5]    G-kdom.批量梯度下降(BGD)、随机梯度下降(SGD)、小批量梯度下降(MBGD)[G/OL].知乎,2019(2019-7-12).<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/72929546">https://zhuanlan.zhihu.com/p/72929546</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">胡小宁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://1905060202.github.io/2021/11/30/%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1-%E4%BB%A5%E4%BA%BA%E4%BA%8B%E6%8B%9B%E8%81%98%E4%B8%BA%E4%BE%8B%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/">http://1905060202.github.io/2021/11/30/%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1-%E4%BB%A5%E4%BA%BA%E4%BA%8B%E6%8B%9B%E8%81%98%E4%B8%BA%E4%BE%8B%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://1905060202.github.io" target="_blank">胡小宁的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><a class="post-meta__tags" href="/tags/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/">课程设计</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD/">计算智能</a></div><div class="post_share"><div class="social-share" data-image="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/11/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%A4%8D%E4%B9%A0/"><img class="prev-cover" src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">大数据技术原理与应用复习</div></div></a></div><div class="next-post pull-right"><a href="/2021/11/29/%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1-%E5%9F%BA%E4%BA%8E%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB/"><img class="next-cover" src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计算智能课程设计(基于感知机的鸢尾花分类)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/11/29/计算智能课程设计-基于感知机的鸢尾花分类/" title="计算智能课程设计(基于感知机的鸢尾花分类)"><img class="cover" src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-29</div><div class="title">计算智能课程设计(基于感知机的鸢尾花分类)</div></div></a></div><div><a href="/2021/12/02/计算智能课程设计-基于传递闭包的模糊聚类/" title="计算智能课程设计(基于传递闭包的模糊聚类)"><img class="cover" src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-02</div><div class="title">计算智能课程设计(基于传递闭包的模糊聚类)</div></div></a></div><div><a href="/2021/06/30/人工智能练习题/" title="人工智能练习题"><img class="cover" src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-30</div><div class="title">人工智能练习题</div></div></a></div><div><a href="/2021/07/17/粒计算综述/" title="粒计算综述"><img class="cover" src="/images/%E5%B0%81%E9%9D%A2/%E7%B2%92%E8%AE%A1%E7%AE%97%E7%BB%BC%E8%BF%B0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-17</div><div class="title">粒计算综述</div></div></a></div><div><a href="/2021/06/22/操作系统课程设计-二/" title="操作系统课程设计(二)-Linux进程管理"><img class="cover" src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-22</div><div class="title">操作系统课程设计(二)-Linux进程管理</div></div></a></div><div><a href="/2021/06/22/操作系统课程设计-三/" title="操作系统课程设计(三)-Linux进程间通信"><img class="cover" src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-22</div><div class="title">操作系统课程设计(三)-Linux进程间通信</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/you.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">胡小宁</div><div class="author-info__description">虽千万人吾往矣</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">90</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">58</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/1905060202"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/1905060202" target="_blank" title="关注我"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">四海之内皆兄弟</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A5%E4%BA%BA%E4%BA%8B%E6%8B%9B%E8%81%98%E4%B8%BA%E4%BE%8B%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">以人事招聘为例的误差反向传播算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84"><span class="toc-number">2.1.</span> <span class="toc-text">实验目的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86"><span class="toc-number">2.2.</span> <span class="toc-text">背景知识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">2.3.</span> <span class="toc-text">示例代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9"><span class="toc-number">2.4.</span> <span class="toc-text">实验内容</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%A2%98"><span class="toc-number">2.4.1.</span> <span class="toc-text">第一题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%A2%98"><span class="toc-number">2.4.2.</span> <span class="toc-text">第二题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%A2%98"><span class="toc-number">2.4.3.</span> <span class="toc-text">第三题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.4.3.1.</span> <span class="toc-text">批量梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.4.3.2.</span> <span class="toc-text">随机梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.4.3.3.</span> <span class="toc-text">小批量梯度下降</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E9%A2%98"><span class="toc-number">2.4.4.</span> <span class="toc-text">第四题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%B6%89%E5%8F%8A%E5%88%B0%E7%9A%84%E5%AE%9E%E9%AA%8C%E8%AF%AD%E6%B3%95%E7%9F%A5%E8%AF%86"><span class="toc-number">2.5.</span> <span class="toc-text">实验涉及到的实验语法知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#numpy-dot-%E5%87%BD%E6%95%B0%E7%9A%84%E7%94%A8%E6%B3%95-1"><span class="toc-number">2.5.1.</span> <span class="toc-text">numpy.dot()函数的用法 [1]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid-%E5%87%BD%E6%95%B0%E7%9A%84%E7%94%A8%E6%B3%95%E5%8F%8A%E6%84%8F%E4%B9%89-2"><span class="toc-number">2.5.2.</span> <span class="toc-text">sigmoid()函数的用法及意义 [2]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Numpy%E4%B8%AD%E7%9A%84shape%E5%87%BD%E6%95%B0%E7%9A%84%E7%94%A8%E6%B3%95%E8%AF%A6%E8%A7%A3-3"><span class="toc-number">2.5.3.</span> <span class="toc-text">Numpy中的shape函数的用法详解 [3]</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.5.3.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%A6%E8%A7%A3"><span class="toc-number">2.5.3.2.</span> <span class="toc-text">详解</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%AD%E6%96%87%E8%A7%A3%E9%87%8A"><span class="toc-number">2.5.3.2.1.</span> <span class="toc-text">中文解释</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8B%B1%E6%96%87%E8%A7%A3%E9%87%8A"><span class="toc-number">2.5.3.2.2.</span> <span class="toc-text">英文解释</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NumPy-%E5%88%87%E7%89%87%E5%92%8C%E7%B4%A2%E5%BC%95-4"><span class="toc-number">2.5.4.</span> <span class="toc-text">NumPy 切片和索引 [4]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-5"><span class="toc-number">2.5.5.</span> <span class="toc-text">批量梯度下降[5]</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">2.5.5.1.</span> <span class="toc-text">批量梯度下降法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">2.5.5.2.</span> <span class="toc-text">随机梯度下降法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">2.5.5.3.</span> <span class="toc-text">小批量梯度下降法</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/12/02/%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1-%E5%9F%BA%E4%BA%8E%E4%BC%A0%E9%80%92%E9%97%AD%E5%8C%85%E7%9A%84%E6%A8%A1%E7%B3%8A%E8%81%9A%E7%B1%BB/" title="计算智能课程设计(基于传递闭包的模糊聚类)"><img src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算智能课程设计(基于传递闭包的模糊聚类)"/></a><div class="content"><a class="title" href="/2021/12/02/%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1-%E5%9F%BA%E4%BA%8E%E4%BC%A0%E9%80%92%E9%97%AD%E5%8C%85%E7%9A%84%E6%A8%A1%E7%B3%8A%E8%81%9A%E7%B1%BB/" title="计算智能课程设计(基于传递闭包的模糊聚类)">计算智能课程设计(基于传递闭包的模糊聚类)</a><time datetime="2021-12-02T05:05:37.000Z" title="发表于 2021-12-02 13:05:37">2021-12-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/01/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%B5%81%E9%87%8F%E8%AE%A1%E6%95%B0/" title="课程设计流量计数"><img src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="课程设计流量计数"/></a><div class="content"><a class="title" href="/2021/12/01/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%B5%81%E9%87%8F%E8%AE%A1%E6%95%B0/" title="课程设计流量计数">课程设计流量计数</a><time datetime="2021-12-01T12:12:28.000Z" title="发表于 2021-12-01 20:12:28">2021-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/01/%E8%93%9D%E7%89%99%E6%8A%80%E6%9C%AF/" title="蓝牙技术"><img src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="蓝牙技术"/></a><div class="content"><a class="title" href="/2021/12/01/%E8%93%9D%E7%89%99%E6%8A%80%E6%9C%AF/" title="蓝牙技术">蓝牙技术</a><time datetime="2021-12-01T10:22:58.000Z" title="发表于 2021-12-01 18:22:58">2021-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/11/30/Typora%EF%BC%8C%E9%97%AD%E6%BA%90%E7%9A%84%E5%A5%BD%E8%BD%AF%E4%BB%B6/" title="Typora，闭源的好软件"><img src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Typora，闭源的好软件"/></a><div class="content"><a class="title" href="/2021/11/30/Typora%EF%BC%8C%E9%97%AD%E6%BA%90%E7%9A%84%E5%A5%BD%E8%BD%AF%E4%BB%B6/" title="Typora，闭源的好软件">Typora，闭源的好软件</a><time datetime="2021-11-30T08:56:54.000Z" title="发表于 2021-11-30 16:56:54">2021-11-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/11/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%A4%8D%E4%B9%A0/" title="大数据技术原理与应用复习"><img src="/images/%E5%B0%81%E9%9D%A2/%E5%9F%8E%E5%B8%821.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大数据技术原理与应用复习"/></a><div class="content"><a class="title" href="/2021/11/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%A4%8D%E4%B9%A0/" title="大数据技术原理与应用复习">大数据技术原理与应用复习</a><time datetime="2021-11-30T02:40:52.000Z" title="发表于 2021-11-30 10:40:52">2021-11-30</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 胡小宁</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --></body></html>